{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E0i0UDN4g5ND",
        "NgmApvbzaVbJ",
        "tmEnIstgaust"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ee76fe590c444ffab4557cbeff51e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39c818f824e54f64a05960475b2cc5cb",
              "IPY_MODEL_7f7e8067341345f498c541a8f2da6d94",
              "IPY_MODEL_bdad5e4dd86344a39bb5e444028906c2"
            ],
            "layout": "IPY_MODEL_0d27a31a6c1041e68ce5c1f74e211f00"
          }
        },
        "39c818f824e54f64a05960475b2cc5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c52dbd6f3c048e784da05de8f8626b5",
            "placeholder": "​",
            "style": "IPY_MODEL_a9ee85f9e3114ac29ecb719c961bc6bd",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "7f7e8067341345f498c541a8f2da6d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_539c87ae47f94059bc45ffd10e85117d",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69d09be4142d40bbae35bbba62af9618",
            "value": 385
          }
        },
        "bdad5e4dd86344a39bb5e444028906c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0014a9b4464825b04fe652c90dead6",
            "placeholder": "​",
            "style": "IPY_MODEL_60c08930cfaf4b7a8ebea065c791b318",
            "value": " 385/385 [00:00&lt;00:00, 5.87kB/s]"
          }
        },
        "0d27a31a6c1041e68ce5c1f74e211f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c52dbd6f3c048e784da05de8f8626b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9ee85f9e3114ac29ecb719c961bc6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "539c87ae47f94059bc45ffd10e85117d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d09be4142d40bbae35bbba62af9618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb0014a9b4464825b04fe652c90dead6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60c08930cfaf4b7a8ebea065c791b318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "429164828f2c4199923a6e83973b3b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_524bfe5d959d4409afb824bdd33d9502",
              "IPY_MODEL_5b14cf4d7f314c10b9569d9a2ed36c43",
              "IPY_MODEL_27fec2f4952e453aa4f9c203530d5382"
            ],
            "layout": "IPY_MODEL_23aba64e222f460f8c9ad91e687e1a22"
          }
        },
        "524bfe5d959d4409afb824bdd33d9502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2981d59ecf974b35b07859a2969638a2",
            "placeholder": "​",
            "style": "IPY_MODEL_3672aa1b7353486bba084e6fc8ac95af",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "5b14cf4d7f314c10b9569d9a2ed36c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faeb6097f7754ae9a03c652799064918",
            "max": 133483028,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fbbdfa8e4754920aa04b858e3ccaab5",
            "value": 133483028
          }
        },
        "27fec2f4952e453aa4f9c203530d5382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d457e17258446c97323184d62bbd65",
            "placeholder": "​",
            "style": "IPY_MODEL_1c05b40ba87c49b9aabd694abd3546c9",
            "value": " 133M/133M [00:03&lt;00:00, 40.4MB/s]"
          }
        },
        "23aba64e222f460f8c9ad91e687e1a22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2981d59ecf974b35b07859a2969638a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3672aa1b7353486bba084e6fc8ac95af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faeb6097f7754ae9a03c652799064918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fbbdfa8e4754920aa04b858e3ccaab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40d457e17258446c97323184d62bbd65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c05b40ba87c49b9aabd694abd3546c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFUijIS4H3A6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292bff49-21c5-4f36-ff6b-2d9ab9998ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1-cp38-cp38-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.0.1) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.0.1) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim==4.0.1) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rank_bm25) (1.21.6)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# Allowed to make changes.\n",
        "import collections\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import timeit\n",
        "from ast import literal_eval\n",
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,AutoModel,AutoConfig\n",
        "from transformers import AutoTokenizer,AutoModelForQuestionAnswering, TrainingArguments, Trainer,AutoConfig,AutoModel\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "from transformers import PreTrainedModel,PretrainedConfig\n",
        "!pip install gensim==4.0.1\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import common_texts\n",
        "\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "!pip install rank_bm25\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use gdrive temporarily, replace with wget"
      ],
      "metadata": {
        "id": "x5iUDzxQLUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCvKz8g5LdvT",
        "outputId": "0c110198-d974-488f-df12-9f37bf8d3493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TASK1_CKPT = '/content/drive/MyDrive/InterIIT-DevRev/miniLM/task1_minilm'\n",
        "TASK2_CKPT = '/content/drive/MyDrive/InterIIT-DevRev/miniLM/task2_minilm_ckpt-5076'#'/content/drive/MyDrive/InterIIT-DevRev/checkpoint-5076'\n",
        "TASK2_CKPT_FT = '/content/drive/MyDrive/InterIIT-DevRev/miniLM/checkpoint-1200'\n",
        "TEST_DATA_PATH = '/content/drive/MyDrive/InterIIT-DevRev/data/Task2dataSet_test.csv'\n",
        "FULL_DATA = '/content/drive/MyDrive/InterIIT-DevRev/data/augment_data.csv'"
      ],
      "metadata": {
        "id": "npDtFuuCLTMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed to make changes.\n",
        "PARAGRAPHS_PATH = '/content/drive/MyDrive/InterIIT-DevRev/data/10th_data/paragraphs.csv'\n",
        "para_df = pd.read_csv(PARAGRAPHS_PATH)\n",
        "paragraph_ds = para_df['paragraph'].tolist()\n",
        "class BM_retriever():\n",
        "  def __init__(self,paragraphs,para_ids=None):\n",
        "    self.paras = paragraphs\n",
        "    # self.para_ids = para_ids\n",
        "    print(\"Pre-processing paragraphs\")\n",
        "    tokenized_corpus= [preprocess_string(t) for t in self.paras]\n",
        "    print(\"Loading BM\")\n",
        "    self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "  def retrieve(self, question,k=10):\n",
        "    tokenized_query = preprocess_string(question)\n",
        "    doc_scores = self.bm25.get_scores(tokenized_query)\n",
        "    pred_para_ids = np.argsort(doc_scores, axis=0)[-k:][::-1]\n",
        "    return pred_para_ids, [self.paras[p] for p  in pred_para_ids[:k]]\n",
        "\n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(384, 384)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(384, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "class MiniLMQA(PreTrainedModel):\n",
        "    def __init__(self,config: PretrainedConfig):\n",
        "        # super(DistillBERTQA, config).__init__()\n",
        "        super().__init__(config)\n",
        "        self.MiniLM = AutoModel.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\") #DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.qa_outputs = torch.nn.Linear(384, 2)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None,start_positions=None,end_positions=None,return_dict=None):\n",
        "        minilm_output = self.MiniLM(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = minilm_output[0]\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        logits = self.qa_outputs(hidden_states)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()  # (bs, max_query_len)\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()  # (bs, max_query_len)\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + minilm_output[1:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=minilm_output.hidden_states,\n",
        "            attentions=minilm_output.attentions,\n",
        "        )\n",
        "\n",
        "############# Task1 model #############\n",
        "print(\"Loading task1 model...\")\n",
        "task1_model = DistillBERTClass()\n",
        "task1_tokenizer = AutoTokenizer.from_pretrained(TASK1_CKPT)\n",
        "task1_statedict = torch.load(TASK1_CKPT + '/pytorch_distilbert.bin',map_location=torch.device('cpu'))\n",
        "task1_model.load_state_dict(task1_statedict.state_dict())\n",
        "print(\"Loaded task1 model\")\n",
        "########################################\n",
        "\n",
        "\n",
        "############# Task2 model #############\n",
        "print(\"Loading task2 model...\")\n",
        "task2_tokenizer = AutoTokenizer.from_pretrained(TASK2_CKPT_FT)\n",
        "task2_config = AutoConfig.from_pretrained(TASK2_CKPT)\n",
        "task2_model = MiniLMQA(task2_config)\n",
        "task2_state_dict = torch.load(TASK2_CKPT + '/pytorch_model.bin',map_location=torch.device('cpu'))\n",
        "task2_model.load_state_dict(task2_state_dict)\n",
        "print(\"Loaded task2 model\")\n",
        "########################################\n",
        "\n",
        "############# Combined model #############\n",
        "# for now dictionary and seperate loading\n",
        "global_model = {'task1':{'model':task1_model,'tokenizer':task1_tokenizer},'task2':{'model':task2_model,'tokenizer':task2_tokenizer}}\n",
        "########################################\n",
        "\n",
        "############ BM model #############\n",
        "print(\"Loading BM model...\")\n",
        "BM_model = BM_retriever(paragraph_ds)\n",
        "print(\"Loaded BM model\")\n",
        "############ BM model #############\n",
        "\n",
        "\n",
        "def answerable(model,tokenizer,question,context):\n",
        "  concat_str = question +' <sep> '+ context\n",
        "  inputs = tokenizer.encode_plus(\n",
        "            concat_str,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_tensors = 'pt'\n",
        "        )\n",
        "  model_inputs = inputs\n",
        "  del model_inputs['token_type_ids']\n",
        "  pred = torch.argmax(model(**model_inputs),dim=1)\n",
        "  if(pred == 1):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def get_answer(model,tokenizer,question,context):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\",max_length=512,truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    model_inputs = inputs\n",
        "    del model_inputs['token_type_ids']\n",
        "    output = model(**model_inputs)\n",
        "\n",
        "    answer_start_scores, answer_end_scores,_ = model(**inputs)\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "def run_prediction(model,question,context):\n",
        "  if(answerable(model = model['task1']['model'],tokenizer = model['task1']['tokenizer'],question = question,context = context)):\n",
        "    return get_answer(model = model['task2']['model'],tokenizer = model['task2']['tokenizer'],question = question,context = context)\n",
        "  else:\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "s88yeOjL12Gv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "0ee76fe590c444ffab4557cbeff51e9f",
            "39c818f824e54f64a05960475b2cc5cb",
            "7f7e8067341345f498c541a8f2da6d94",
            "bdad5e4dd86344a39bb5e444028906c2",
            "0d27a31a6c1041e68ce5c1f74e211f00",
            "7c52dbd6f3c048e784da05de8f8626b5",
            "a9ee85f9e3114ac29ecb719c961bc6bd",
            "539c87ae47f94059bc45ffd10e85117d",
            "69d09be4142d40bbae35bbba62af9618",
            "cb0014a9b4464825b04fe652c90dead6",
            "60c08930cfaf4b7a8ebea065c791b318",
            "429164828f2c4199923a6e83973b3b0d",
            "524bfe5d959d4409afb824bdd33d9502",
            "5b14cf4d7f314c10b9569d9a2ed36c43",
            "27fec2f4952e453aa4f9c203530d5382",
            "23aba64e222f460f8c9ad91e687e1a22",
            "2981d59ecf974b35b07859a2969638a2",
            "3672aa1b7353486bba084e6fc8ac95af",
            "faeb6097f7754ae9a03c652799064918",
            "2fbbdfa8e4754920aa04b858e3ccaab5",
            "40d457e17258446c97323184d62bbd65",
            "1c05b40ba87c49b9aabd694abd3546c9"
          ]
        },
        "outputId": "c45e9bf8-40e8-407b-c998-c8c274d35528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading task1 model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ee76fe590c444ffab4557cbeff51e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "429164828f2c4199923a6e83973b3b0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded task1 model\n",
            "Loading task2 model...\n",
            "Loaded task2 model\n",
            "Loading BM model...\n",
            "Pre-processing paragraphs\n",
            "Loading BM\n",
            "Loaded BM model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed to make changes.\n",
        "def get_theme_model(theme):\n",
        "  # Here, you can load and return a model which is fine tuned for a theme.\n",
        "  # In case, you only have a global model, you can return that.\n",
        "  return global_model"
      ],
      "metadata": {
        "id": "p8RcyC4O2DFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def retrieve_contestants_BM(question,k=3):\n",
        "  ret_para,ret_para_ids = BM_model.retrieve(question,k)\n",
        "  return (ret_para,ret_para_ids)\n",
        "\n",
        "def pred_theme_ans(questions, theme_model, pred_out):\n",
        "  theme = questions[0][\"theme\"]\n",
        "  for question in questions:\n",
        "    ans = {}\n",
        "    ans[\"question_id\"] = question[\"id\"]\n",
        "    questions_str = question['question']\n",
        "    contensting_paras_ids,contensting_paras = retrieve_contestants_BM(question=questions_str,k=10)\n",
        "    for i in range(len(contensting_paras)):\n",
        "      para = contensting_paras[i]\n",
        "      para_id = contensting_paras_ids[i]\n",
        "      our_prediction = run_prediction(model = theme_model,question=questions_str,context=para)\n",
        "      ans[\"answers\"] = our_prediction\n",
        "      if(our_prediction == \"\"):\n",
        "        ans[\"paragraph_id\"] = -1\n",
        "      else:\n",
        "        ans[\"paragraph_id\"] = para_id\n",
        "        break\n",
        "    pred_out.append(ans)"
      ],
      "metadata": {
        "id": "LuH3XOWwLjY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes.\n",
        "\n",
        "# All theme prediction.\n",
        "questions = json.loads(pd.read_csv(\"/content/sample_input_question_1.csv\").to_json(orient=\"records\"))\n",
        "theme_intervals = json.loads(pd.read_csv(\"/content/sample_theme_interval.csv\").to_json(orient=\"records\"))\n",
        "pred_out = []\n",
        "theme_inf_time = {}\n",
        "for theme_interval in theme_intervals:\n",
        "  theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n",
        "  theme = theme_ques[0][\"theme\"]\n",
        "  # Load model fine-tuned for this theme.\n",
        "  theme_model = get_theme_model(theme)\n",
        "  execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n",
        "  theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n",
        "pred_df = pd.DataFrame.from_records(pred_out)\n",
        "pred_df.fillna(value='', inplace=True)\n",
        "# Write prediction to a CSV file. Teams are required to submit this csv file.\n",
        "pred_df.to_csv('/content/sample_output_prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "V4MGougaIImX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0441099-e5f0-4382-b066-4e1793c0df69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes.\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def calc_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def calc_max_f1(predicted, ground_truths):\n",
        "  max_f1 = 0\n",
        "  if len(ground_truths) == 0:\n",
        "    return len(predicted) == 0\n",
        "  for ground_truth in ground_truths:\n",
        "    f1 = calc_f1(predicted, ground_truth)\n",
        "    max_f1 = max(max_f1, f1)\n",
        "  return max_f1"
      ],
      "metadata": {
        "id": "lempoIKsIJ_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes.\n",
        "\n",
        "# Evaluation methodology.\n",
        "metrics = {}\n",
        "pred = pd.read_csv(\"sample_output_prediction.csv\")\n",
        "pred.fillna(value='', inplace=True)\n",
        "truth = pd.read_csv(\"sample_ground_truth.csv\")\n",
        "truth.fillna(value='', inplace=True)\n",
        "truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n",
        "truth.answers = truth.answers.apply(literal_eval)\n",
        "questions = pd.read_csv(\"sample_input_question.csv\")\n",
        "for idx in pred.index:\n",
        "  q_id = pred[\"question_id\"][idx]\n",
        "  q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n",
        "  theme = q_rows[\"theme\"]\n",
        "  predicted_paragraph = pred[\"paragraph_id\"][idx]\n",
        "  predicted_ans = pred[\"answers\"][idx]\n",
        "\n",
        "  if theme not in metrics.keys():\n",
        "    metrics[theme] = {\"true_positive\": 0, \"true_negative\": 0, \"total_predictions\": 0, \"f1_sum\": 0}\n",
        "\n",
        "  truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n",
        "  truth_paragraph_id = [ int(i) for i in truth_row[\"paragraph_id\"] ]\n",
        "  if predicted_paragraph in truth_paragraph_id:\n",
        "    # Increase TP for that theme.\n",
        "    metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n",
        "  # -1 prediction in case there is no paragraph which can answer the query.\n",
        "  if predicted_paragraph == -1 and truth_row[\"paragraph_id\"] == []:\n",
        "    # Increase TN.\n",
        "    metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n",
        "  # Increase total predictions for that theme.\n",
        "  metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n",
        "  f1 = calc_max_f1(predicted_ans, truth_row[\"answers\"])\n",
        "  metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1"
      ],
      "metadata": {
        "id": "-HA5KB3RIL4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT allowed to make changes.\n",
        "\n",
        "# Final score.\n",
        "inf_time_threshold = 1000.0 # milliseconds.\n",
        "final_para_score = 0.0\n",
        "final_qa_score = 0.0\n",
        "# Weight would stay hidden from teams.\n",
        "theme_weights = {\"Kubernetes\": 0.5, \"ChatGPT\": 0.4, \"Football world cup\": 0.1}\n",
        "for theme in metrics:\n",
        "  inf_time_score = 1.0\n",
        "  metric = metrics[theme]\n",
        "  para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"]\n",
        "  qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n",
        "  avg_inf_time = theme_inf_time[theme] / metric[\"total_predictions\"]\n",
        "  if avg_inf_time > inf_time_threshold:\n",
        "    inf_time_score = inf_time_threshold / avg_inf_time\n",
        "  final_qa_score += theme_weights[theme] * inf_time_score * qa_score\n",
        "  final_para_score += theme_weights[theme] * inf_time_score * para_score\n",
        "print (final_para_score)\n",
        "print (final_qa_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuojd-v8INyd",
        "outputId": "d045c987-2fd7-454f-b013-c1e4196332b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7\n",
            "0.5666666666666667\n"
          ]
        }
      ]
    }
  ]
}